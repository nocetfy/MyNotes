import{_ as r,X as i,Y as n,$ as t}from"./framework-1ee2252c.js";const a={},p=t('<h1 id="稳定性" tabindex="-1"><a class="header-anchor" href="#稳定性" aria-hidden="true">#</a> 稳定性</h1><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/9a9d6d1604478693f28938d2104a376c.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="背景介绍" tabindex="-1"><a class="header-anchor" href="#背景介绍" aria-hidden="true">#</a> 背景介绍</h2><p>​ 在移动互联网时代，用户群的积累比之前更容易，但同样，也会因为糟糕的用户体验，而快速流失用户，哪怕是号称独一无二的 12306 网站，也在不断优化系统来提升用户体验；而在后移动互联网的物联网时代，软件工程师需要和硬件工程师配合，来保证提供的服务稳定和可靠。对，<strong>我们的产品就是为了实现用户价值，并提供非凡用户体验</strong>！</p><p>​ 那么，什么是系统稳定性？这里我们引用百度百科的定义：<strong>系统稳定性是指系统要素在外界影响下表现出的某种稳定状态</strong>。为了方便，本文阐述的系统主要指软件系统。那么如何衡量系统稳定性的高与低呢？一个常用的指标就是服务可用时长占比，占比越高说明系统稳定性也越高，如果我们拿一整年的数据来看，常见的 4 个 9（99.99%）意味着我们系统提供的服务全年的不可用时长只有 52 分钟！它其实是一个综合指标，为什么这么说？因为我们在服务可用的定义上会有一些差别，常见的服务可用包括：<strong>服务无异常、服务响应时间低、服务有效（逻辑正确）、服务能正常触发</strong> 等。</p><h2 id="故障源的分类" tabindex="-1"><a class="header-anchor" href="#故障源的分类" aria-hidden="true">#</a> 故障源的分类</h2><p>系统的故障源一般可以分为两大类，一类是人为因素，另一类是自然因素。常见人为因素导致的故障如下：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/3b3f07c2312fc5c26ae4068f2e107c02.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>​ 人为因素我们要尽可能的 <strong>事前</strong>（故障发生前）避免，因为这些原因引发的事故很可能会导致数据丢失或错乱、资金受损等较严重后果，而且除了重启或修复后重新上线外没有过多有效的止损手段。人为因素导致的故障往往会导致软件工程师的内心受到严重打击，工作和专业能力受到质疑，造成“人财两空”的后果，“我拼了老命来产出，结果却给自己挖了个坑”是故障责任人内心的真实写照。</p><p>​ 我们再来说说自然因素，自然因素受很多客观因素的影响，往往不受控制，无法避免。常见的自然因素导致的故障如下：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/dcb505f07db5fa137f26e8ce0d3ad905.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>自然原因导致的故障可大可小，虽然无法避免，但由于没有第一责任人，避免了“人性拷问”，软件工程师可以和运维部、安全部的同学协作起来处理故障。</p><h2 id="稳定性建设四要素" tabindex="-1"><a class="header-anchor" href="#稳定性建设四要素" aria-hidden="true">#</a> 稳定性建设四要素</h2><p>​ “如果事情有变坏的可能，不管这种可能性有多小，它总会发生。”，残酷的墨菲定律预示着我们对自己系统提供的服务不要太乐观，接下来，我们说说如何建设系统稳定性，人为因素的根源一方面是专业能力不足，经验不足，另一方面很多都是无心之失，所以需要通过流程、规范来保住“底线”，减少人为因素导致的故障，而自然因素导致的故障往往具有突发性，需要联合多个团队协作来解决故障。</p><p><strong>稳定性建设四要素：人、工具、预案和目标</strong>。</p><h3 id="第一要素-人" tabindex="-1"><a class="header-anchor" href="#第一要素-人" aria-hidden="true">#</a> 第一要素：人</h3><p>我们先来说“人”这一要素，它需要回答如下 5 个问题：</p><ul><li><strong>谁应该参与稳定性建设？</strong></li><li><strong>如何降低犯错的概率？</strong></li><li><strong>如何提高稳定性意识？</strong></li><li><strong>如何定责？</strong></li><li><strong>如何激励？</strong></li></ul><p>​ 稳定性建设工作需要老板支持，它的实施一般需要 <strong>开发、测试、运维、安全</strong> 还有 <strong>产品</strong> 等同学参与，而且主导方应该是开发、测试和运维。确定了参与方后，就可以做关键的一步：“<strong>参与稳定性建设的每个团队都需要在 OKR 中背负一部分稳定性指标</strong>”，这也是为什么说稳定性建设工作需要老板支持，因为和绩效考核相关。</p><p>​ <strong>稳定性工作，规范先行</strong>。OKR 的部分只是让各参与方在稳定性方面工作的投入变成合规化，平时如何去参与稳定性建设还得“有迹可循”，对于开发和测试来说就是要根据公司的当前技术体系去建设 <strong>开发规范、提测规范、测试规范、上线规范、复盘规范</strong> 等。我们拿和软件开发最相关的开发规范来说，开发规范是对开发人员的要求，让开发人员知道什么是必须要做的、什么是推荐的、什么是应该避免的。通常开发规范至少应该包括如下几个部分：</p><p>​ <strong>编码规范</strong>：对外接口命名方式、统一异常父类、业务异常码规范、对外提供服务不可用是抛异常还是返回错误码、统一第三方库的版本、哪些场景必须使用内部公共库、埋点日志怎么打、提供统一的日志、监控切面实现等，编码规范除了能规范开发的编码行为、避免犯一些低级错误和踩一些重复的坑外，另一个好处是让新入职的同学能快速了解公司的编码原则，这点对编码快速上手很重要。</p><p>​ 这里再重点说一下为什么要统一异常父类和业务异常码，例如虽然不同模块（这里的模块指的是能独立部署的项目）可能有不同的异常父类，比如订单模块的异常父类是 OrderException、交易支付模块的异常父类是 TradeException，而 OrderException 和 TradeException 的父类是 BizException（当然 BizException 是定义在一个通用共公共库中的），而我们也需要去统一异常码，比如 200 代表正确的返回码，异常的返回码是 6 位数字（前 3 位代表模块，后 3 位代表异常类型），有了统一的异常父类和异常码后，很多切面就都可以由公共库来做了，比如统一的监控、统一的出入口日志打印，统一的异常拦截，压测标识透传、特殊的字段埋点等，千万别小看这些，这些能在未来持续提升研发效率，降低稳定性工作成本。</p><p>​ <strong>公共库使用规范</strong>：为了能对通用功能进行定制化改造和封装，公司内部肯定会有一些公共库，例如日志库、HTTP 库、线程池库、监控埋点库等，这些库都“久经考验”，已经被证实是有效且可靠的，这些就应该强制使用，当然为了适应业务的发展，这些公共库也应该进行迭代和升级。</p><p>​ <strong>项目结构规范</strong>：为了贯彻标准的项目结构，一方面我们需要为各种类型项目通过“项目脚手架”来创建标准的项目结构原型，然后基于这个项目原型来进行开发，统一的项目结构一个最显著的好处是让开发能快速接手和了解项目，这种对于团队内维护多个项目很重要，人员能进行快速补位。</p><p>​ <strong>数据库规范</strong>：数据库连接资源堪比 CPU 资源，现在的应用都离不开数据库，而且通常数据库都属于核心资源，一旦数据库不可用，应用都没有太有效的止损手段，所以在数据库规范里，库名、表名、索引、字段、分库分表的一些规范都必须明确。</p><p>​ 这里特别提一点，就是分表数量不要用 2 的幂（比如 1024 张表，很多人认为使用 2 的幂分表数在计算分表时用位运算会更快，但这个开销相比数据库操作其实可以忽略），而应该用质数（比如最接近 1024 的质数应该是 1019），采用质数分表数能让数据分的更均匀。</p><p>​ 这会引发另一个问题，那就是我们有这些规范，那么如何让开发来知晓和遵守？一方面是设定合理的奖惩机制（例如由于没有遵守规范而引发的线上事故要严惩），另一方面就是——考试！没错，就是考试，将这些规范和历史的线上事故整理成试题，让新老开发定期去考试，考试是一种传统的考核机制，我们可以把规范和公共库的更新部分，也及时加入到考试试题中，来督促大伙及时学习。有了 OKR、规范和考核机制，加上我们定期宣导，相信各成员的稳定性意识会有显著提高。</p><p>​ 事故定责一般是比较复杂的过程，除非事故原因非常简单明了，但实际上事故原因常常涉及多个团队，如果责任分摊不合理，难免会引发跨团队的争吵，合理的做法是引入第三方稳定性团队来干预，例如滴滴的星辰花团队，星辰花会撰写定责指南，并制定一些相关流程机制。</p><p>​ 当然，如果达成稳定性年度目标，也应该对这些团队进行适当表彰。</p><h3 id="第二要素-工具" tabindex="-1"><a class="header-anchor" href="#第二要素-工具" aria-hidden="true">#</a> 第二要素：工具</h3><p>​ 工具意味着手段，要做好稳定性建设，强大的支持工具和平台是不可缺少的，常见的工具和平台包括：日志采集分析检索平台（例如滴滴的 Arius）、监控告警平台（例如滴滴的 Odin Metrics）、分布式追踪系统（例如 Google 的 Dapper、滴滴的把脉平台）、自动化打包部署平台（例如滴滴的 One Experience）、服务降级系统（例如滴滴的 SDS）、预案平台（例如滴滴的 911 预案平台）、根因定位平台（记录所有故障发生前所有系统变更事件）、放火平台等。</p><p>强大的工具能回答如下 3 个关键问题：</p><ul><li><strong>我们能做什么？</strong></li><li><strong>我们能做到什么程度？</strong></li><li><strong>如何降低稳定性工作成本？</strong></li></ul><p>​ 工具本质上是手段，它能降低我们在稳定性工作上投入的成本，例如有了监控告警平台，我们就不需要专人时刻盯着日志或大盘，有了分布式追踪系统，问题定位会更有效率，有了降级系统，一些故障能自动控制和恢复，不用我们再上线一次。要想做好稳定性工作，工具必不可少，没有工具，稳定性建设总是低效的。</p><p>​ 其实公司内建的公共库也属于工具的一种，像滴滴内部的公共库，业务系统接入 Odin Metrics 和把脉几乎不要做额外的工作（当然接入把脉需要提日志采集工单，头疼），千万不要吝啬在工具方面的投入，很多开源框架可以拿来用或拿来参考，工具和平台可以内部进行互通和联动，这样可以建成一站式的稳定性工作平台。</p><h3 id="第三要素-预案" tabindex="-1"><a class="header-anchor" href="#第三要素-预案" aria-hidden="true">#</a> 第三要素：预案</h3><p>紧急预案是我们在故障发生时的行动指南，这在故障可能涉及到多个团队、故障进展需要周知到多个团队时特别有用。</p><p>完善的紧急预案能回答如下 4 个问题：</p><ul><li><strong>故障发生时我们该做什么？</strong></li><li><strong>谁来指挥？</strong></li><li><strong>谁来决策？</strong></li><li><strong>我们如何善后？</strong></li></ul><p>​ 当一个不那么容易定位的故障发生时，你应该做的第一件事应该是什么？这在不同公司、同一个公司同一个团队的不同成员恐怕都会给出一个不同的答案，而在滴滴内部，我们大多会第一时间通知团队内其他成员、Leader（寻求帮助）和客服、上游业务开发等可能的影响方 （问题周知）。</p><p>​ 当这一步做完以后，一般就会有一部分同学加入问题排查和止损，然而介入的人多了，排查和止损的效率不一定会成比例的提升，这时候协调者很重要，协调者要避免介入的同学在做重复工作，协调者还需要持续和客服、上游业务开发等影响方沟通（我们曾经就经历过由于问题排查问题进度没有及时有效和业务方沟通，业务方将故障升级的 case）。对于排查问题和止损的同学来说，要操作某个开关，有可能还要去查代码看开关的名字是什么，还有可能关掉一个功能需要操作多个开关，这些在紧急时刻都有可能由于慌乱而出错。而且什么条件下才能操作开关，谁能决定应不应该操作开关，恐怕在当时很难去做最正确的事情，而这一切，没错，都应该提前写到预案中！！！</p><p>​ 紧急预案一般要包含如下内容：</p><ul><li>故障发生时应该通知哪些人或团队。</li><li>如何选出协调者，什么情况下该选出协调者。</li><li>协调者的职责有哪些。</li><li>需要操作开关时，谁有权利决策。</li><li>常见故障以及对应的止损方式。</li><li>止损的原则是什么，什么是最重要的。善后方案谁来拍板。</li></ul><p>​ 预案很重要，完备的预案能降低故障定位和止损的时间，提升协作效率。</p><h3 id="第四要素-目标" tabindex="-1"><a class="header-anchor" href="#第四要素-目标" aria-hidden="true">#</a> 第四要素：目标</h3><p>​ 如何衡量稳定性建设工作是有价值的？如何考核稳定性建设工作达没达标、做的好不好？这些都能在稳定性建设的目标中找到答案。</p><p>​ 稳定性建设的目标主要用来回答如下 2 个问题：</p><ul><li><strong>稳定性工作的价值是什么？</strong></li><li><strong>稳定性工作如何考核？</strong></li></ul><p>​ 稳定性建设工作的价值不仅需要团队所有成员认可，更重要的是需要老板的认可，没有老板的认可，稳定性建设工作只是团队内部的“小打小闹”，难以去跨团队来体系化运作。</p><p>​ 稳定性建设工作的年度目标可以拿服务可用时长占比来定，也可以拿全年故障等级和次数来定，像滴滴这边，星辰花将故障等级分成了 P0 至 P5 六个等级，P0、P1、P2 属于重大事故，是需要消耗服务不可用时长的（根据全年定的服务可用时长占比指标来计算出某个部门的全年服务不可用总时长），一旦年底某个部门的全年服务不可用时长超过年初设定的阈值，就会有一定的处罚，并影响部门绩效（之前达标也有奖励，但后来奖励取消了）。</p><p>这里做一个汇总：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/5d720a80936dd940c8f6eb8787f0d0dd.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h2 id="稳定性建设四个方向" tabindex="-1"><a class="header-anchor" href="#稳定性建设四个方向" aria-hidden="true">#</a> 稳定性建设四个方向</h2><p>前面我们提到的稳定性建设工作的四个关键点，但对如何落地阐述的并不多，这里结合作者多年的稳定性建设工作经验，谈谈稳定性建设工作的四个方向。放张图来进行总结：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/d9d8d437189c7764f69e099632f438ec.png" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结" aria-hidden="true">#</a> 总结</h2><p>稳定性是一个大投入，做稳定性建设一定要结合公司的实际情况，量入为出，最合适的方案才是最好的方案。结合咱们上述讨论的几点，我们可以画出稳定性建设的房子，如下：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/d73f3d2185fa056883cd8b32c4e3ebf8-20231022102650389.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="提升稳定性的举措" tabindex="-1"><a class="header-anchor" href="#提升稳定性的举措" aria-hidden="true">#</a> 提升稳定性的举措</h2><h3 id="预防" tabindex="-1"><a class="header-anchor" href="#预防" aria-hidden="true">#</a> 预防</h3><h4 id="提升服务稳定" tabindex="-1"><a class="header-anchor" href="#提升服务稳定" aria-hidden="true">#</a> 提升服务稳定</h4><ul><li>服务平滑重启</li><li>分级发布：沙盒、灰度、小流量</li><li>业务隔离(服务分级)</li><li>重要流程分片</li><li>数据库保护</li><li>第三方通道稳定性：银行、公安网...</li><li>容灾机制：文件建立定期备份机制、缓存数据容灾</li><li>冗余部署、故障自动转移</li></ul><h4 id="机房、网络、故障" tabindex="-1"><a class="header-anchor" href="#机房、网络、故障" aria-hidden="true">#</a> 机房、网络、故障</h4><ul><li>链路容灾切换</li><li>链路在线压测</li><li>两地三中心</li><li>DB自动化切换</li><li>流量整形</li><li>服务熔断、降级</li><li>容量评估、自动扩缩容</li></ul><h3 id="监控" tabindex="-1"><a class="header-anchor" href="#监控" aria-hidden="true">#</a> 监控</h3><ul><li><p><strong>日志查询平台</strong>(日志查询，trace跟踪，日志分析等等，可以逐步类似阿里鹰眼)</p></li><li><p><strong>服务统计</strong>(服务日常情况，服务利用率分析等) 出具分析报表</p></li><li><p>服务治理</p></li></ul><blockquote><p>常见的业务监控指标：</p><ul><li>3xx、4xx、5xx</li><li>流量突增突降</li><li>平均耗时</li></ul></blockquote><h3 id="处理" tabindex="-1"><a class="header-anchor" href="#处理" aria-hidden="true">#</a> 处理</h3><ul><li>预案梳理(各种常见故障的预案整理)</li><li><strong>预案演练</strong>(故障定期模拟演练)</li><li>快速扩容缩容演练</li><li>预案自动化</li><li>故障复盘</li></ul><h3 id="分阶段的措施" tabindex="-1"><a class="header-anchor" href="#分阶段的措施" aria-hidden="true">#</a> 分阶段的措施</h3><h4 id="单服务稳定性" tabindex="-1"><a class="header-anchor" href="#单服务稳定性" aria-hidden="true">#</a> 单服务稳定性</h4><p><strong>关键字：开关可控、单一职责、服务隔离、异常兜底、监控发现！</strong></p><p>对于稳定性来说，抛开整体系统架构设计，单就每个业务域服务的稳定性也是非常的重要。</p><p>只有每个业务环节都稳如泰山，才可以保障整个稳定性。单服务的稳定可以从以下几个方面来进行：</p><p><strong>1、禁用设计</strong>：应该提供控制具体功能是否开启可用的配置，在相应的功能服务出现故障时，快速下线局部功能，以保证整体服务的可用性；</p><p><strong>2、必要的缓存</strong>：缓存是解决并发的利器，可以有效的提高系统的吞吐量。按照业务以及技术的纬度必要时可以增加多级缓存来保证其命中率；</p><p><strong>3、接口无状态性</strong>：服务接口应该是无状态的，当前接口访问不应该依赖上层接口的状态逻辑；</p><p><strong>4、接口单一职责性</strong>：对于核心功能的接口，不应该过多的耦合不属于它的功能。如果一个接口做的事情太多应做拆分，保证单接口的稳定性和快速响应；</p><p><strong>5、第三方服务隔离性</strong>：任何依赖于第三方的服务（不论接口还是中间件等），都应该做到熔断和降级，不能有强耦合的依赖；</p><p><strong>6、业务场景兜底方案</strong>：核心业务场景需要做到完整的兜底方法，从前端到后端都应该有兜底措施；</p><p><strong>7、服务监控与及时响应</strong>：每个服务应该做好对应的监控工作，如有异常应及时响应，不应累积。</p><h4 id="集群稳定性" tabindex="-1"><a class="header-anchor" href="#集群稳定性" aria-hidden="true">#</a> 集群稳定性</h4><p><strong>关键字：系统架构、部署发布、限流熔断、监控体系、压测机制！</strong></p><p>对于集群维度的稳定性来说，稳定性保障会更加复杂。单服务是局部，集群是全局。一个见微知著，一个高瞻远瞩。</p><p><strong>1、合理的系统架构</strong>：合理的系统架构是稳定的基石；</p><p><strong>2、小心的代码逻辑</strong>：代码时刻都要小心，多担心一点这里会不会有性能问题，那里会不会出现并发，代码就不会有多少问题；</p><p><strong>3、优秀的集群部署</strong>：一台机器永远会有性能瓶颈，优秀的集群部署，可以将一台机器的稳定放大无限倍，是高并发与大流量的保障；</p><p><strong>4、科学的限流熔断</strong>：高并发来临时，科学的限流和熔断是系统稳定的必要条件；</p><p><strong>5、精细的监控体系</strong>：没有监控体系，你永远不会知道你的系统到底有多少隐藏的问题和坑，也很难知道瓶颈在哪里；</p><p><strong>6、强悍的压测机制</strong>：压测是高并发稳定性的试金石，能提前预知高并发来临时，系统应该出现的模样；</p><p><strong>7、胆小的开发人员</strong>：永远需要一群胆小的程序员，他们讨厌bug，害怕error，不放过每一个波动，不信任所有的依赖。</p><h4 id="稳定性专项" tabindex="-1"><a class="header-anchor" href="#稳定性专项" aria-hidden="true">#</a> 稳定性专项</h4><p>专项指的是<strong>针对某些特定场景下的特定问题而梳理出对应的方案</strong>。下面是针对一些常见的稳定性专项的概述：</p><p><strong>1、预案</strong>：分为定时预案和紧急预案，定时预案是大促常规操作对于一系列开关的编排，紧急预案是应对突发情况的特殊处理，都依赖于事前梳理；</p><p><strong>2、预热</strong>：分为JIT代码预热和数据预热，阿里内部有专门的一个产品负责这块，通过存储线上的常态化流量或者热点流量进行回放来提前预热，</p><p>起源于某年双十一零点的毛刺问题，原因是访问了数据库的冷数据rt增高导致的一系列上层限流，现在预热已经成了大促之前的一个必要流程。</p><p><strong>3、强弱依赖</strong>:梳理强弱依赖是一个偏人肉的过程，但是非常重要，这是一个系统自查识别潜在风险点并为后续整理开关限流预案和根因分析的一个重要参考，</p><p>阿里内部有一个强弱依赖检测的平台，通过对测试用例注入RPC调用的延迟或异常来观察链路的依赖变化，自动梳理出强弱依赖关系。</p><p><strong>4、限流降级熔断</strong>:应对突发流量防止请求超出自身处理能力系统被击垮的必要手段；</p><p><strong>5、监控告警&amp;链路追踪</strong>:监控分为业务监控、系统监控和中间件监控和基础监控，作为线上问题发现和排查工具，重要性不言而喻。</p><h4 id="稳定性建设" tabindex="-1"><a class="header-anchor" href="#稳定性建设" aria-hidden="true">#</a> 稳定性建设</h4><p>稳定性建设，就和基础技术建设一样，是一个<strong>长期迭代和不断调整的过程</strong>，业内常见的稳定性建设类型，主要有如下几种：</p><p><strong>1、容量规划</strong>：个人感觉容量规划在大厂里也并没有做的很好，更多依赖的是业务方自己拍脑袋，然后全链路压测期间验证，不够就再加机器。</p><p><strong>2、混沌工程</strong>：混沌工程是近几年比较火的名词，通过不断给系统找麻烦来验证并完善系统能力，阿里在这块花了很大的精力建设红蓝军对抗攻防，进行定期和不定期的演练，</p><p>最后以打分的形式来给各个部门系统做排名，除了系统层面的故障演练外还有资金演练，篡改线上sql语句制造资损来测试业务监控纠错的能力，通过制造小错来避免大错。</p><p><strong>3、流量调度</strong>：通过metric秒级监控和聚类算法实时找出异常单机来降低RPC流量权重，提升集群整体吞吐能力减少异常请求。</p><p><strong>4、容灾&amp;异地多活</strong>：起源于15年某施工队将光纤挖断带来的支付宝故障，由此出来的三地五中心和单元化架构，异地多活本身的成本比较高，</p><p>然后又存在数据同步的延时问题和切流带来的脏数据问题，对于业务和技术都有比较高的要求。常见的容灾有如下几种：</p><p>1）缓存挂掉，集群重启缓存预热如何处理？本地缓存，多级缓存是否可以替代？</p><p>2）分布式锁，是否有开关一键切换？比如：ZK/ETCD编写的分布式锁；</p><p>3）大促峰值流量，如何防止外部ddos攻击？如何识别流量类型？</p><p>4）资源隔离：资源隔离，服务分组，流量隔离；</p><p>5）高可用思想：避免单点设计！</p><p>6）容错：容错上游，防御下游。容错主要需要注意如下几点：</p><p>6-1：外部依赖的地方都要做熔断，避免雪崩；</p><p>6-2：对于依赖我们的上游要限流，防止上游突发流量超过自己系统能够扛住的最大QPS；</p><p>6-3：对于下游既要评估好接口超时时间，防止下游接口超时异常导致自己系统被拖累；</p><p>6-4：下游的接口要考虑各种异常情况，需要考虑中间状态，通过引入柔性事务，确保数据最终一致。</p><h4 id="异地多活" tabindex="-1"><a class="header-anchor" href="#异地多活" aria-hidden="true">#</a> 异地多活</h4><p><strong>异地多活的本质，是数据中心架构的演进</strong>。</p><p><strong>1）演进</strong>：单机房——双机房——异地灾备——异地多活；</p><p><strong>2）定义</strong>：分多个地域、多个数据中心运行线上的业务，并且每个IDC均提供在线服务；</p><p><strong>3）优点</strong>：弹性扩展能力、流量就近接入、灵活调度、提升可用性与用户体验、容灾；</p><p><strong>4）步骤</strong>：</p><p>4-1：基础设施：机房之间专线互联，保证网络质量稳定；</p><p>4-2：持久存储：一主三从，主IDC同步复制，异地IDC异步复制；</p><p>4-3：中间件：DB、MQ、分布式存储；</p><p>4-4：应用部署：根据应用域划分，不同应用部署在不同地域，保持亲缘性；</p><p>4-5：流量接入与调度：网络协议兼容，DNS，动态调度用户就近访问；</p><p>4-6：监控与运维保障：专线实时监控，确保发生故障时可以触发Failover（失效备援）和流量调度。</p><h4 id="稳定性思考" tabindex="-1"><a class="header-anchor" href="#稳定性思考" aria-hidden="true">#</a> 稳定性思考</h4><p><strong>关键字：阶段工作、角色转变！</strong></p><p>稳定性建设是一个演进的阶段性过程，主要分为三个阶段：</p><p><strong>1、发现问题解决问题</strong>：当问题较多时候就很被动，很多时候我们通过不断完善监控来确保我们来快速定位问题，但仍处于被动的一方；</p><p><strong>2、主动寻找问题</strong>：混沌工程、破坏性测试、极限压测、红蓝对抗等手段，一方作为创造问题方不断挑战系统极限，另一方见招拆招快速修复。</p><p><strong>3、角色转变</strong>：这个过程中会积累很多处理问题的经验，不断完善系统健壮性，争取在用户发现问题前消灭于萌芽中。角色转变，变被动为主动。</p>',136),e=[p];function o(s,g){return i(),n("div",null,e)}const d=r(a,[["render",o],["__file","稳定性.html.vue"]]);export{d as default};
