import{_ as e,X as i,Y as a,$ as t}from"./framework-1ee2252c.js";const r={},n=t(`<h1 id="中间件" tabindex="-1"><a class="header-anchor" href="#中间件" aria-hidden="true">#</a> 中间件</h1><p>[toc]</p><h2 id="zookeeper" tabindex="-1"><a class="header-anchor" href="#zookeeper" aria-hidden="true">#</a> zookeeper</h2><h3 id="概述" tabindex="-1"><a class="header-anchor" href="#概述" aria-hidden="true">#</a> 概述</h3><p>ZooKeeper 基础知识基本分为三大模块：</p><ul><li>数据模型</li><li>ACL 权限控制</li><li>Watch 监控</li></ul><p>其中，数据模型是最重要的，很多 ZooKeeper 中典型的应用场景都是利用这些基础模块实现的。比如我们可以利用数据模型中的临时节点和 Watch 监控机制来实现一个发布订阅的功能。</p><p>ZooKeeper 中的数据模型是一种树形结构，非常像电脑中的文件系统，有一个根文件夹，下面还有很多子文件夹。ZooKeeper 的数据模型也具有一个固定的根节点（/），我们可以在根节点下创建子节点，并在子节点下继续创建下一级节点。ZooKeeper 树中的每一层级用斜杠（/）分隔开，且只能用绝对路径（如“get /work/task1”）的方式查询 ZooKeeper 节点，而不能使用相对路径。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/CgqCHl6yL9uAbpHYAABF_GHyGNc950.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h4 id="持久节点" tabindex="-1"><a class="header-anchor" href="#持久节点" aria-hidden="true">#</a> 持久节点</h4><p>​ 这种节点也是在 ZooKeeper 最为常用的，几乎所有业务场景中都会包含持久节点的创建。之所以叫作持久节点是因为一旦将节点创建为持久节点，该数据节点会一直存储在 ZooKeeper 服务器上，即使创建该节点的客户端与服务端的会话关闭了，该节点依然不会被删除。如果我们想删除持久节点，就要显式调用 delete 函数进行删除操作。</p><h4 id="临时节点" tabindex="-1"><a class="header-anchor" href="#临时节点" aria-hidden="true">#</a> 临时节点</h4><p>​ 该节点的一个最重要的特性就是临时性。所谓临时性是指，如果将节点创建为临时节点，那么该节点数据不会一直存储在 ZooKeeper 服务器上。当创建该临时节点的客户端会话因超时或发生异常而关闭时，该节点也相应在 ZooKeeper 服务器上被删除。同样，我们可以像删除持久节点一样主动删除临时节点。</p><h4 id="有序节点" tabindex="-1"><a class="header-anchor" href="#有序节点" aria-hidden="true">#</a> 有序节点</h4><p>​ 其实有序节点并不算是一种单独种类的节点，而是在之前提到的持久节点和临时节点特性的基础上，增加了一个节点有序的性质。所谓节点有序是说在我们创建有序节点的时候，ZooKeeper 服务器会自动使用一个单调递增的数字作为后缀，追加到我们创建节点的后边。例如一个客户端创建了一个路径为 works/task- 的有序节点，那么 ZooKeeper 将会生成一个序号并追加到该节点的路径后，最后该节点的路径为 works/task-1。通过这种方式我们可以直观的查看到节点的创建顺序。</p><hr><h3 id="发布订阅模式" tabindex="-1"><a class="header-anchor" href="#发布订阅模式" aria-hidden="true">#</a> 发布订阅模式</h3><p>ZooKeeper 的客户端也可以通过 Watch 机制来订阅当服务器上某一节点的数据或状态发生变化时收到相应的通知，我们可以通过向 ZooKeeper 客户端的构造方法中传递 Watcher 参数的方式实现：</p><div class="language-java line-numbers-mode" data-ext="java"><pre class="language-java"><code><span class="token keyword">new</span> <span class="token class-name">ZooKeeper</span><span class="token punctuation">(</span><span class="token class-name">String</span> connectString<span class="token punctuation">,</span> <span class="token keyword">int</span> sessionTimeout<span class="token punctuation">,</span> <span class="token class-name">Watcher</span> watcher<span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/Ciqc1F61IL-AEQuUAABdpaAsy2k628.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><h4 id="例子" tabindex="-1"><a class="header-anchor" href="#例子" aria-hidden="true">#</a> 例子</h4><p>​ 在系统开发的过程中会用到各种各样的配置信息，如数据库配置项、第三方接口、服务地址等，这些配置操作在我们开发过程中很容易完成，但是放到一个大规模的集群中配置起来就比较麻烦了。通常这种集群中，我们可以用配置管理功能自动完成服务器配置信息的维护，利用ZooKeeper 的发布订阅功能就能解决这个问题。</p><p>​ 我们可以把诸如数据库配置项这样的信息存储在 ZooKeeper 数据节点中。如图中的 /confs/data_item1。服务器集群客户端对该节点添加 Watch 事件监控，当集群中的服务启动时，会读取该节点数据获取数据配置信息。而当该节点数据发生变化时，ZooKeeper 服务器会发送 Watch 事件给各个客户端，集群中的客户端在接收到该通知后，重新读取节点的数据库配置信息。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/CgqCHl61INaAJeAEAAA8lZ8lpbE688.png" alt="image" tabindex="0" loading="lazy"><figcaption>image</figcaption></figure><p><strong>Watch 具有一次性，所以当我们获得服务器通知后要再次添加 Watch 事件。</strong></p><h3 id="选举" tabindex="-1"><a class="header-anchor" href="#选举" aria-hidden="true">#</a> 选举</h3><p>zookeeper集群中公共有三种角色，分别是<code>leader</code>，<code>follower</code>，<code>observer</code>。</p><table><thead><tr><th style="text-align:left;">角色</th><th style="text-align:left;">描述</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>leader</strong></td><td style="text-align:left;">主节点，又名领导者。用于写入数据，通过选举产生，如果宕机将会选举新的主节点。</td></tr><tr><td style="text-align:left;"><strong>follower</strong></td><td style="text-align:left;">子节点，又名追随者。用于实现数据的读取。同时他也是主节点的备选节点，并拥有投票权。</td></tr><tr><td style="text-align:left;"><strong>observer</strong></td><td style="text-align:left;">次级子节点，又名观察者。用于读取数据，与follower区别在于没有投票权，不能被选为主节点。并且在计算集群可用状态时不会将observer计算入内。</td></tr></tbody></table><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/640-20230304232448506.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>zk会进行多轮的投票，直到某一个节点的票数大于或等于半数以上，在3个节点中，总共会进行2轮的投票：</p><ul><li>第一轮，每个节点启动时投票给自己，那这样zk1，zk2，zk3各有一票。</li><li>第二轮，每个节点投票给大于自己myid，那这样zk2启动时又获得一票。加上自己给自己投的那一票。总共有2票。2票大于了当前节点总数的半数，所以投票终止。zk2当选leader。</li></ul><p>​ ZXID是当前节点最后一次事务的ID。<strong>如果整个集群数据为一致的，那么所有节点的ZXID应该一样。所以zookeeper就通过这个有序的ZXID来确保各个节点之间的数据的一致性，带着之前的问题，如果Leader宕机之后，再重启后，会去和目前的Leader去比较最新的ZXID，如果节点的ZXID比最新Leader里的ZXID要小，那么就会去同步数据。zk的选举过程会优先考虑ZXID大的节点</strong>。</p><p>客户端写入数据提交流程大致为：leader接受到客户端的写请求，然后同步给各个子节点：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/640-20230304232337913.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>如果客户端连接的是<code>follower</code>:</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/640-20230304232410621.png" alt="图片" tabindex="0" loading="lazy"><figcaption>图片</figcaption></figure><p>当 ZooKeeper 集群选举出 Leader 服务器后，ZooKeeper 集群中的服务器就开始更新自己的角色信息，<strong>除被选举成 Leader 的服务器之外，其他集群中的服务器角色变更为 Following。</strong></p><h4 id="脑裂问题" tabindex="-1"><a class="header-anchor" href="#脑裂问题" aria-hidden="true">#</a> 脑裂问题</h4><p>​ 集群发现master挂掉（网络导致的假死），进行master切换，并且通知client，切换和通知client都需要时间，导致有的client连接到老的master，有的连接到新的master，此时连接不同master的client更新，就导致数据不一致，</p><p>​ 主要原因是Zookeeper集群和Zookeeper client判断超时并不能做到完全同步，也就是说可能一前一后，如果是集群先于client发现那就会出现上面的情况。同时，在发现并切换后通知各个客户端也有先后快慢。一般出现这种情况的几率很小，需要master与Zookeeper集群网络断开但是与其他集群角色之间的网络没有问题，还要满足上面那些情况，但是一旦出现就会引起很严重的后果，数据不一致。</p><p>​ 在slaver切换的时候不在检查到老的master出现问题后马上切换，而是在休眠一段足够的时间，确保老的master已经获知变更并且做了相关的shutdown清理工作了然后再注册成为master就能避免这类问题了，这个休眠时间一般定义为与Zookeeper定义的超时时间就够了，但是这段时间内系统不可用了。</p><p>​ ZK的过半机制一定程度上也减少了脑裂情况的出现，起码不会同时出现三个leader。ZK中的Epoch机制（时钟）每次选举都是递增+1，当通信时需要判断epoch是否一致，小于自己的则抛弃，大于自己则重置自己，通过epoch大小来拒绝旧的leader发起的请求，这个时候，重新恢复通信的老的leader节点会进入恢复模式，与新的leader节点做数据同步。</p><h3 id="eureka和zookeeper的区别" tabindex="-1"><a class="header-anchor" href="#eureka和zookeeper的区别" aria-hidden="true">#</a> Eureka和zookeeper的区别</h3><ul><li>zookeeper是CP模型，Eureka是AP模型</li><li>eureka集群中的各个节点是平等的地位，peer to peer对等通信。这是一种<strong>去中心化</strong>的架构，在这种架构风格中，节点通过彼此互相注册来提高可用性，每个节点都可被视为其他节点的副本。zookeeper集群是存在leader和follower关系的，也就是一主多从。</li></ul><hr><h2 id="elasticsearch" tabindex="-1"><a class="header-anchor" href="#elasticsearch" aria-hidden="true">#</a> ElasticSearch</h2><h3 id="概述-1" tabindex="-1"><a class="header-anchor" href="#概述-1" aria-hidden="true">#</a> 概述</h3><p>ElasticSearch是一款非常强大的、基于Lucene的开源搜索及分析引擎；它是一个实时的分布式搜索分析引擎，它能让你以前所未有的速度和规模，去探索你的数据。它被用作<strong>全文检索</strong>、<strong>结构化搜索</strong>、<strong>分析</strong>以及这三个功能的组合。除了搜索，结合Kibana、Logstash、Beats开源产品，Elastic Stack（简称ELK）还被广泛运用在大数据近实时分析领域，包括：<strong>日志分析</strong>、<strong>指标监控</strong>、<strong>信息安全</strong>等。它可以帮助你<strong>探索海量结构化、非结构化数据，按需创建可视化报表，对监控数据设置报警阈值，通过使用机器学习，自动识别异常状况</strong>。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/es-introduce-1-3.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/es-introduce-1-1.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="搜索引擎原理" tabindex="-1"><a class="header-anchor" href="#搜索引擎原理" aria-hidden="true">#</a> 搜索引擎原理</h3><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659002.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li><p>内容爬取，停顿词过滤</p><p>比如一些无用的像&quot;的&quot;，“了”之类的语气词/连接词</p></li><li><p>内容分词，提取关键词</p></li><li><p>根据关键词建立<strong>倒排索引</strong></p></li><li><p>用户输入关键词进行搜索</p></li></ul><h3 id="倒排索引" tabindex="-1"><a class="header-anchor" href="#倒排索引" aria-hidden="true">#</a> 倒排索引</h3><p>设想一个关于搜索的场景，假设我们要搜索一首诗句内容中带“前”字的古诗，</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659000.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>建立倒排索引之后，我们上述的查询需求会变成什么样子</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659004.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="概念" tabindex="-1"><a class="header-anchor" href="#概念" aria-hidden="true">#</a> 概念</h4><h4 id="term" tabindex="-1"><a class="header-anchor" href="#term" aria-hidden="true">#</a> term</h4><p>在 ES 中，关键词被称为 <strong>term</strong>。</p><h4 id="postings-list" tabindex="-1"><a class="header-anchor" href="#postings-list" aria-hidden="true">#</a> postings list</h4><p>还是用上面的例子，<code>{静夜思, 望庐山瀑布}</code>是 &quot;前&quot; 这个 term 所对应列表。在 ES 中，这些被描述为所有包含特定 term 文档的 id 的集合。由于整型数字 integer 可以被高效压缩的特质，integer 是最适合放在 postings list 作为文档的唯一标识的，ES 会对这些存入的文档进行处理，转化成一个唯一的整型 id。</p><p>再说下这个 id 的范围，在存储数据的时候，在每一个 shard 里面，ES 会将数据存入不同的 segment，这是一个比 shard 更小的分片单位，这些 segment 会定期合并。在每一个 segment 里面都会保存最多 2^31 个文档，每个文档被分配一个唯一的 id，从<code>0</code>到<code>(2^31)-1</code>。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659005.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="内部结构" tabindex="-1"><a class="header-anchor" href="#内部结构" aria-hidden="true">#</a> 内部结构</h4><p>在实际生产场景中，比如 ES 最常用的日志分析，日志内容进行分词之后，可以得到多少的 term？</p><p>那么如何快速的在海量 term 中查询到对应的 term 呢？遍历一遍显然是不现实的。</p><h5 id="term-dictionary" tabindex="-1"><a class="header-anchor" href="#term-dictionary" aria-hidden="true">#</a> term dictionary</h5><p>于是乎就有了 <strong>term dictionary</strong>，ES 为了能快速查找到 term，将所有的 term 排了一个序，二分法查找。是不是感觉有点眼熟，这不就是 MySQL 的索引方式的，直接用 B+树建立索引词典指向被索引的数据。</p><h5 id="term-index" tabindex="-1"><a class="header-anchor" href="#term-index" aria-hidden="true">#</a> term index</h5><p>但是问题又来了，你觉得 Term Dictionary 应该放在哪里？肯定是放在内存里面吧？磁盘 io 那么慢。就像 MySQL 索引就是存在内存里面了。</p><p>但是如果把整个 term dictionary 放在内存里面会有什么后果呢？内存爆了...</p><p>别忘了，ES 默认可是会对全部 text 字段进行索引，必然会消耗巨大的内存，为此 ES 针对索引进行了深度的优化。在保证执行效率的同时，尽量缩减内存空间的占用。于是乎就有了 <strong>term index</strong>。</p><p>Term index 从数据结构上分类算是一个“Trie 树”，也就是我们常说的字典树。这是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。</p><p>​ 这棵树不会包含所有的 term，它包含的是 term 的一些前缀（这也是字典树的使用场景，公共前缀）。通过 term index 可以快速地定位到 term dictionary 的某个 offset，然后从这个位置再往后顺序查找。就想右边这个图所表示的。（怎么样，像不像我们查英文字典，我们定位 S 开头的第一个单词，或者定位到 Sh 开头的第一个单词，然后再往后顺序查询）</p><p>lucene 在这里还做了两点优化，一是 term dictionary 在磁盘上面是分 block 保存的，一个 block 内部利用<strong>公共前缀压缩</strong>，比如都是 Ab 开头的单词就可以把 Ab 省去。二是 term index 在内存中是以 FST（finite state transducers）的数据结构保存的。</p><p>FST 有两个优点：</p><ul><li>空间占用小。通过对词典中单词前缀和后缀的重复利用，压缩了存储空间</li><li>查询速度快。O(len(str)) 的查询时间复杂度。</li></ul><p>OK，现在我们能得到 lucene 倒排索引大致是个什么样子的了。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659003-20230305032344081.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="关于-postings-list-的一些巧技" tabindex="-1"><a class="header-anchor" href="#关于-postings-list-的一些巧技" aria-hidden="true">#</a> 关于 postings list 的一些巧技</h4><p>在实际使用中，postings list 还需要解决几个痛点，</p><ul><li>postings list 如果不进行压缩，会非常占用磁盘空间，</li><li>联合查询下，如何快速求交并集（intersections and unions）</li></ul><p>对于如何压缩，可能会有人觉得没有必要，”posting list 不是已经只存储文档 id 了吗？还需要压缩？”，但是如果在 posting list 有百万个 doc id 的情况，压缩就显得很有必要了。（比如按照朝代查询古诗？），至于为啥需要求交并集，ES 是专门用来搜索的，肯定会有很多联合查询的需求吧 （AND、OR）。</p><p>按照上面的思路，我们先将如何压缩。</p><h5 id="压缩" tabindex="-1"><a class="header-anchor" href="#压缩" aria-hidden="true">#</a> 压缩</h5><h6 id="frame-of-reference" tabindex="-1"><a class="header-anchor" href="#frame-of-reference" aria-hidden="true">#</a> Frame of Reference</h6><p>在 lucene 中，要求 postings lists 都要是有序的整形数组。这样就带来了一个很好的好处，可以通过 增量编码（delta-encode）这种方式进行压缩。</p><p>比如现在有 id 列表 <code>[73, 300, 302, 332, 343, 372]</code>，转化成每一个 id 相对于前一个 id 的增量值（第一个 id 的前一个 id 默认是 0，增量就是它自己）列表是<code>[73, 227, 2, 30, 11, 29]</code>。<strong>在这个新的列表里面，所有的 id 都是小于 255 的，所以每个 id 只需要一个字节存储</strong>。</p><p>实际上 ES 会做的更加精细</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659006.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>它会把所有的文档分成很多个 block，每个 block 正好包含 256 个文档，然后单独对每个文档进行增量编码，计算出存储这个 block 里面所有文档最多需要多少位来保存每个 id，并且把这个位数作为头信息（header）放在每个 block 的前面。这个技术叫 <strong>Frame of Reference</strong>。</p><p>上图也是来自于 ES 官方博客中的一个示例（假设每个 block 只有 3 个文件而不是 256）。</p><p>FOR 的步骤可以总结为：</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659007-20230305032440360.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>进过最后的位压缩之后，整型数组的类型从固定大小 (8,16,32,64 位)4 种类型,扩展到了[1-64] 位共 64 种类型。</p><p>通过以上的方式可以极大的节省 posting list 的空间消耗，提高查询性能。不过 ES 为了提高 filter 过滤器查询的性能，还做了更多的工作，那就是<strong>缓存</strong>。</p><h6 id="roaring-bitmaps-for-filter-cache" tabindex="-1"><a class="header-anchor" href="#roaring-bitmaps-for-filter-cache" aria-hidden="true">#</a> Roaring Bitmaps (for filter cache)</h6><p>在 ES 中，可以使用 filters 来优化查询，filter 查询只处理文档是否匹配与否，不涉及文档评分操作，查询的结果可以被缓存。</p><p>对于 filter 查询，es 提供了 filter cache 这种特殊的缓存，filter cache 用来存储 filters 得到的结果集。缓存 filters 不需要太多的内存，它只保留一种信息，即哪些文档与 filter 相匹配。同时它可以由其它的查询复用，极大地提升了查询的性能。</p><p>我们上面提到的 Frame Of Reference 压缩算法对于 postings list 来说效果很好，但对于需要存储在内存中的 filter cache 等不太合适。</p><p>filter cache 会存储那些经常使用的数据，针对 filter 的缓存就是为了加速处理效率，对压缩算法要求更高。</p><p>对于这类 postings list，ES 采用不一样的压缩方式。那么让我们一步步来。</p><p>首先我们知道 postings list 是 Integer 数组，具有压缩空间。</p><p>假设有这么一个数组，我们第一个压缩的思路是什么？用位的方式来表示，每个文档对应其中的一位，也就是我们常说的位图，bitmap。</p><p>它经常被作为索引用在数据库、查询引擎和搜索引擎中，并且位操作（如 and 求交集、or 求并集）之间可以并行，效率更好。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659008-20230305032444381.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>但是，位图有个很明显的缺点，不管业务中实际的元素基数有多少，它占用的内存空间都恒定不变。也就是说不适用于稀疏存储。业内对于稀疏位图也有很多成熟的压缩方案，lucene 采用的就是<strong>roaring bitmaps</strong>。</p><p>我这里用简单的方式描述一下这个压缩过程是怎么样，</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659009.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>将 doc id 拆成高 16 位，低 16 位。对高位进行聚合 (以高位做 key，value 为有相同高位的所有低位数组)，根据低位的数据量 (不同高位聚合出的低位数组长度不相同)，使用不同的 container(数据结构) 存储。</p><ul><li>len&lt;4096 ArrayContainer 直接存值</li><li>len&gt;=4096 BitmapContainer 使用 bitmap 存储</li></ul><p>分界线的来源：value 的最大总数是为<code>2^16=65536</code>. 假设以 bitmap 方式存储需要 <code>65536bit=8kb</code>,而直接存值的方式，一个值 2 byte，4K 个总共需要<code>2byte*4K=8kb</code>。所以当 value 总量 &lt;4k 时,使用直接存值的方式更节省空间。</p><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/1460000037659010.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>空间压缩主要体现在:</p><ul><li>高位聚合 (假设数据中有 100w 个高位相同的值,原先需要 <code>100w*2byte</code>,现在只要 <code>1*2byte</code>)</li><li>低位压缩</li></ul><p>缺点就在于位操作的速度相对于原生的 bitmap 会有影响。</p><blockquote><p>这就是 trade-off 呀。平衡的艺术。</p></blockquote><h5 id="联合查询" tabindex="-1"><a class="header-anchor" href="#联合查询" aria-hidden="true">#</a> 联合查询</h5><p>讲完了压缩，我们再来讲讲联合查询。</p><p>先讲简单的，如果查询有 filter cache，那就是直接拿 filter cache 来做计算，也就是说位图来做 AND 或者 OR 的计算。</p><p>如果查询的 filter 没有缓存，那么就用 skip list 的方式去遍历磁盘上的 postings list。</p><figure><img src="https://segmentfault.com/img/remote/1460000037659011" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>以上是三个 posting list。我们现在需要把它们用 AND 的关系合并，得出 posting list 的交集。首先选择最短的 posting list，逐个在另外两个 posting list 中查找看是否存在，最后得到交集的结果。遍历的过程可以跳过一些元素，比如我们遍历到绿色的 13 的时候，就可以跳过蓝色的 3 了，因为 3 比 13 要小。</p><p>用 skip list 还会带来一个好处，还记得前面说的吗，postings list 在磁盘里面是采用 FOR 的编码方式存储的</p><blockquote><p>会把所有的文档分成很多个 block，每个 block 正好包含 256 个文档，然后单独对每个文档进行增量编码，计算出存储这个 block 里面所有文档最多需要多少位来保存每个 id，并且把这个位数作为头信息（header）放在每个 block 的前面。</p></blockquote><p>因为这个 FOR 的编码是有解压缩成本的。利用 skip list，<strong>除了跳过了遍历的成本，也跳过了解压缩这些压缩过的 block 的过程，从而节省了 cpu</strong>。</p><hr><h3 id="深度翻页" tabindex="-1"><a class="header-anchor" href="#深度翻页" aria-hidden="true">#</a> 深度翻页</h3><p>在使用 Elasticsearch 过程中，应尽量避免大翻页的出现。</p><p>正常翻页查询都是从 from 开始 size 条数据，这样就需要在每个分片中查询打分排名在前面的 from+size 条数据。协同节点收集每个分配的前 from+size 条数据。协同节点一共会受到 N*(from+size) 条数据，然后进行排序，再将其中 from 到 from+size 条数据返回出去。如果 from 或者 size 很大的话，导致参加排序的数量会同步扩大很多，最终会导致 CPU 资源消耗增大。</p><p>可以通过使用 Elasticsearch scroll 和 scroll-scan 高效滚动的方式来解决这样的问题。</p><p>也可以结合实际业务特点，文档 id 大小如果和文档创建时间是一致有序的，可以以文档 id 作为分页的偏移量，并将其作为分页查询的一个条件。</p><hr><h2 id="apollo" tabindex="-1"><a class="header-anchor" href="#apollo" aria-hidden="true">#</a> Apollo</h2><h3 id="好处" tabindex="-1"><a class="header-anchor" href="#好处" aria-hidden="true">#</a> 好处</h3><ul><li><strong>统一管理不同环境、不同集群的配置</strong></li><li><strong>配置修改实时生效（热发布）</strong></li><li><strong>版本发布管理</strong></li><li><strong>灰度发布</strong></li><li><strong>权限管理、发布审核、操作审计</strong></li><li><strong>客户端配置信息监控</strong></li><li>提供多语言客户端</li></ul><h3 id="架构" tabindex="-1"><a class="header-anchor" href="#架构" aria-hidden="true">#</a> 架构</h3><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/overall-architecture.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ul><li>Config Service提供配置的读取、推送等功能，服务对象是Apollo客户端</li><li>Admin Service提供配置的修改、发布等功能，服务对象是Apollo Portal（管理界面）</li><li>Config Service和Admin Service都是多实例、无状态部署，所以需要将自己注册到Eureka中并保持心跳</li><li>在Eureka之上我们架了一层Meta Server用于封装Eureka的服务发现接口</li><li>Client通过域名访问Meta Server获取Config Service服务列表（IP+Port），而后直接通过IP+Port访问服务，同时在Client侧会做load balance、错误重试</li><li>Portal通过域名访问Meta Server获取Admin Service服务列表（IP+Port），而后直接通过IP+Port访问服务，同时在Portal侧会做load balance、错误重试</li><li>为了简化部署，我们实际上会把Config Service、Eureka和Meta Server三个逻辑角色部署在同一个JVM进程中</li></ul><h3 id="apollo客户端的实现原理" tabindex="-1"><a class="header-anchor" href="#apollo客户端的实现原理" aria-hidden="true">#</a> Apollo客户端的实现原理</h3><figure><img src="https://raw.githubusercontent.com/nocetfy/image/main/img/client-architecture.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li>客户端和服务端保持了一个长连接，从而能第一时间获得配置更新的推送。</li><li>客户端还会定时从Apollo配置中心服务端拉取应用的最新配置。 <ul><li>这是一个fallback机制，为了防止推送机制失效导致配置不更新</li><li>客户端定时拉取会上报本地版本，所以一般情况下，对于定时拉取的操作，服务端都会返回304 - Not Modified</li><li>定时频率默认为每5分钟拉取一次，客户端也可以通过在运行时指定System Property: <code>apollo.refreshInterval</code>来覆盖，单位为分钟。</li></ul></li><li>客户端从Apollo配置中心服务端获取到应用的最新配置后，会保存在内存中</li><li>客户端会把从服务端获取到的配置在本地文件系统缓存一份 <ul><li>在遇到服务不可用，或网络不通的时候，依然能从本地恢复配置</li></ul></li><li>应用程序可以从Apollo客户端获取最新的配置、订阅配置更新通知。</li></ol><h3 id="配置更新推送原理" tabindex="-1"><a class="header-anchor" href="#配置更新推送原理" aria-hidden="true">#</a> 配置更新推送原理</h3><p>通过Http Long Polling实现的，具体而言：</p><ul><li>客户端发起一个Http请求到服务端</li><li>服务端会保持住这个连接30秒</li><li>如果在30秒内有客户端关心的配置变化，被保持住的客户端请求会立即返回，并告知客户端有配置变化的namespace信息，客户端会据此拉取对应namespace的最新配置</li><li>如果在30秒内没有客户端关心的配置变化，那么会返回Http状态码304给客户端</li><li>客户端在服务端请求返回后会自动重连</li></ul><p>考虑到会有数万客户端向服务端发起长连，在服务端我们使用了async servlet(Spring DeferredResult)来服务Http Long Polling请求。</p><hr>`,149),o=[n];function l(s,c){return i(),a("div",null,o)}const d=e(r,[["render",l],["__file","中间件.html.vue"]]);export{d as default};
